---
title: "Statistical Computing and Simulation: HW1"
author: |  
  | Deparment of Statistics, NCCU  
  | 葉佐晨
  | 112354016@nccu.edu.tw
  |
  | 高崇哲
  | 112354@nccu.edu.tw
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies:
      ctex: UTF8
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: 72
---

# Statistical Computing and Simulation

## Assignment 1, Due March 15/2024

1.  

    (a) Write a computer program using the Mid-Square Method using 6
        digits to generate 10,000 random numbers ranging over
        [0,999999]. Use the Kolmogorov-Smirnov Goodness-of-fit test to
        see if the random numbers that you create are uniformly
        distributed. (Note: You must notify the initial seed number
        used, and you may adapt 0.05 as the $\alpha$ value. Also, you
        may find warning messages for conducting the Goodness-of-fit
        test, and comment on the Goodness-of-fit test.)
    (b) Consider the combination of 3 multiplicative congruential
        generators, i.e.,

    $$u_i=\frac{x_i}{30269}+\frac{y_i}{30307}+\frac{z_i}{30323}\text{ (mod 1)}$$

    with $x_i=171x_{i-1}\text{ (mod 30269)}$,
    $y_i=172y_{i-1}\text{ (mod 30307)}$,
    $z_i=170z_{i-1}\text{ (mod 30323)}$. Compare the results in (a) and
    (b), and discuss your findings.

```{r}
library(magrittr)
library(extraDistr)
```

```{r}
midSquare <- function(size = 1){
  initSample <- function(){sample(0:999999, size = 1)}

  num <- initSample()
  nums <- c(num)
  
  for (i in seq(size)){
    num <- num^2 %>% 
      format(scientific = FALSE) %>% 
      substr(start = 4, stop = 9) %>% 
      ifelse(. == "", initSample(), .) %>% 
      as.integer()
    
    num <- ifelse(num == nums[length(nums)], initSample(), num)
    nums <- c(nums, num)
  }
  
  return(nums)
}
```

```{r}
randSeq <- midSquare(size = 10000)
ks.test(randSeq, "pdunif", 0, 999999)
```

```{r}
LCG <- function(size, mod, mul, seed = NULL){
  nums <- c()
  if (!is.null(seed)){set.seed(seed)}
  
  xyz <- sapply(mod, function(x){sample(0:x, size = 1)})
  
  for (i in seq(size)){
    u <- sum(xyz / mod)
    nums <- c(nums, u)
    xyz <- (xyz * mul) %% mod
  }
  
  return(nums)
}
```

```{r}
mod <- c(30269, 30307, 30323)
mul <- c(172, 171, 170)
randomSeq <- LCG(size = 10000, mod = mod, mul = mul)

ks.test(randomSeq, "punif", 0, 3)
```

2.  

    (a) In class, we often use simulation tools in R, e.g., "sample" or
        "ceiling(runif)," to generate random numbers from 1 to k, where
        k is a natural number. Using graphical tools (such as histogram)
        and statistical tests to check which one is a better tool in
        producing uniform numbers between 1 and k. (Hint: You may check
        if the size of k matters by, for example, assigning k a small or
        big value.)
    (b) Hand calculators often use $U_{n+1}=(\pi+{U_n})^5$ (mod 1) to
        generate random numbers between 0 and 1.Compare the result with
        those in #1, and discuss your finding based on the comparison.

(a).

```{r}
library(ggplot2)
```

```{r}
generate_histogram <- function(data, k, title) {
  histogram <- ggplot(data.frame(x = data), aes(x)) +
    geom_histogram(binwidth = 1, fill = "blue", alpha = 0.5) +
    labs(title = title, x = "Value", y = "Frequency")
  print(histogram)
}

calculate_and_test <- function(data, k) {
  num <- c()
  for (i in 0:(k-1)) {
    c <- data[data <= (1*(i+1)) & data > (1*i)]
    num <- c(num, length(c))
  }
  print(num)
  chisq.test(num, p = rep(1/k, k))
}
```

```{r}
set.seed(123)
sample_data_10 <- sample(1:10, 1000, replace = TRUE)
ceiling_data_10 <- ceiling(runif(1000) * 10)
sample_data_100 <- sample(1:100, 1000, replace = TRUE)
ceiling_data_100 <- ceiling(runif(1000) * 100)
generate_histogram(sample_data_10, 10, "Random 1~10 Generated by 'sample'")
generate_histogram(ceiling_data_10, 10, "Random 1~10 Generated by 'ceiling(runif)'")
generate_histogram(sample_data_100, 100, "Random 1~100 Generated by 'sample'")
generate_histogram(ceiling_data_100, 100, "Random 1~100 Generated by 'ceiling(runif)'")
```

We conducted 1000 random simulations for both the "sample" and "ceiling"
functions within the ranges of 1\~10 and 1\~100, respectively. From the
graphs, we observed that regardless of whether it's the sample or
ceiling function, when operating within a small range, the generated
random numbers are relatively uniform. However, as the range increases,
the uniformity decreases accordingly.

```{r}
calculate_and_test(sample_data_10, 10)
calculate_and_test(ceiling_data_10, 10)
calculate_and_test(sample_data_100, 100)
calculate_and_test(ceiling_data_100, 100)
```

In the simulation with a range of 1\~10, we divided the data into 10
blocks for chi-square tests. Similarly, in the simulation with a range
of 1\~100, we divided the data into 100 blocks for chi-square tests.
With a significance level $\alpha$ = 0.05, none of our four tests
rejected the null hypothesis. Therefore, we conclude that the
performance of the "sample" and "ceiling" functions in this experiment
is similar.

(b).

```{r}
pi_fn<-function(seed,n){
  un<-seed
  a<-c()
  for (i in 1:n){
    un<-(un+pi)^5
    un<-un%%1
    a<-c(a,un)
  }
  return(a)
}
```

```{r}
example5_b_small<-pi_fn(101,100)
example5_b_big<-pi_fn(100,10000)
hist(example5_b_small,main="Pi method of n=100", breaks = 20)
hist(example5_b_big,main="Pi method of n=10000",  breaks = 20)
```

From the graphs, we can observe that when we conduct 100 simulations
using this method, it doesn't quite resemble a uniform distribution.
However, after 10,000 simulations, the differences in values for each
interval significantly decrease, resembling a uniform distribution.

```{r}
ks.test(example5_b_small, "punif", 0, 1)
ks.test(example5_b_big, "punif", 0, 1)
```

From the Kolmogorov-Smirnov test results, it is evident that regardless
of conducting 100 or 10,000 simulations, when $\alpha$ = 0.05, the
method does not reject the null hypothesis. This indicates that the
method performs well in simulating a uniform distribution from 0\~1.
Based on the test results, we believe that this method outperforms
Method #1.

3.  There are several ways for checking the goodness-of-fit for
    empirical data. In specific, there are a lot of normality tests
    available in R. Generate a random sample of size 10, 50, and 100
    from N(0,1) and t-distribution (with degrees 10 and 20) in R. You
    may treat testing random numbers from t-distribution as the power.
    For a level of significance $\alpha$ = 0.05 test, choose at least
    four normality tests in R ("nortest" module) to check if this sample
    is from N(0,1). Tests used can include the Kolmogorov-Smirnov test
    and the Cramer-von Mises test. Note that you need to compare the
    differences among the tests you choose.

```{r}
library(nortest)
```

```{r}
norTest <- function(testList, n, seed = NULL){
  if (!is.null(seed)) {set.seed(seed)}
  
  res <- rbind(
    sapply(testList, function(test){test(rnorm(n))[["p.value"]]}),
    sapply(testList, function(test){test(rt(n, df = 10))[["p.value"]]}),
    sapply(testList, function(test){test(rt(n, df = 20))[["p.value"]]})
  )
  
  colnames(res) <- c(c("ad", "cvm", "lillie", "pearson", "sf"))
  rownames(res) <- c("N(0,1)", "t(df=10)", "t(df=20)")
  return(res)
}

countReject <- function(n_trial, n, alpha){
   replicate(n = n_trial, 
            norTest(testList, n = n) <= alpha) %>% 
    apply(MARGIN = c(1, 2), sum)
}
```

```{r}
n <- c(10, 50, 100)
testList <- c(ad.test, cvm.test, lillie.test, pearson.test, sf.test)
n_trial <- 1000
alpha <- 0.05

```

```{r}
rejList <- lapply(n, function(n){countReject(n_trial = 1000, n = n, alpha = 0.05)})
names(rejList) <- paste0("n=", n)
print(rejList)
```

4.  Write your own R programs to perform Gap test, Permutation test, and
    run test. Then use this program to test if the uniform random
    numbers generated from Minitab (or SAS, SPSS, Excel) and R are
    independent.

```{r}
gap.test=function(data,a,b){
  n=length(data)
  x=c(1:n)*(a<data & data<b)
  x1=x[x>0]
  y=x1[-1]-x1[-length(x1)]-1
  return(table(y))
}

set.seed(123)
uniform_data <- runif(1200)
gap_counts <- gap.test(uniform_data, 0.2, 0.8)
gap_counts
```

We used the "runif" function in R language to generate 1200 values, and
then employed Gap test to check the independence of this sample. In Gap
test, we set the thresholds to 0.2 and 0.8. The results above represent
the frequencies of samples falling within the thresholds and the
distances between them. Since we are proceeding with a chi-square test
next, we merged the frequencies of gaps 5 and 6.

```{r}
result_vector <- numeric()
for (i in c(0:5)) {
  a = 0.6*0.4^i
  result_vector <- c(result_vector, a)
}
result_vector[6] <- 1 - sum(result_vector[1:5])
observation <- c(446, 179,  63,  26,  14, 5) 
chisq.test(observation, p = result_vector)
```

The result from the Gap test shows that when $\alpha$ = 0.05, we do not
reject the null hypothesis.

```{r}
permute.test=function(data,k){
  y=rep(10,k)^c((k-1):0)
  x=matrix(data,ncol=k,byrow=T)
  x1=apply(x,1,rank)
  yy=apply(x1*y,2,sum)
  return(table(yy))
}

permute.test(uniform_data, 3)
```

Next, we utilize a Permutation test. Samples are grouped in sets of 3,
each set sorted by magnitude. The results above indicate the quantity of
various permutations.

```{r}
observation <- c(54, 66,  67,  80,  65, 68)
chisq.test(observation, p = rep(1/6, 6))
```

The result from the Permutation test shows that when $\alpha$ = 0.05, we
do not reject the null hypothesis.

```{r}
run.test <- function(data, base_number) {
  n <- length(data)
  observed_runs <- sum(data > base_number)
  expected_runs <- n / 2
  p_value <- 2*pbinom(observed_runs, size = n, prob = 0.5, lower.tail = TRUE)
  return(list(observed_runs = observed_runs, expected_runs = expected_runs, p_value = p_value))
}

run.test(uniform_data, 0.5)
```

Finally, we conduct a Run test. Similarly, from the test results, it can
be observed that when $\alpha$ = 0.05, we do not reject the null
hypothesis.

```{r}
py_data <- read.csv('random_numbers.csv')
py_data <- py_data[, 1]
gap_counts <- gap.test(py_data, 0.2, 0.8)

result_vector <- numeric()
for (i in c(0:5)) {
  a = 0.6*0.4^i
  result_vector <- c(result_vector, a)
}
result_vector[6] <- 1 - sum(result_vector[1:5])
observation <- c(423, 169,  69,  21,  16, 10) 
chisq.test(observation, p = result_vector)

permute.test(py_data, 3)
observation <- c(71, 69,  58,  77,  68, 57)
chisq.test(observation, p = rep(1/6, 6))

run.test(py_data, 0.5)
```

We also generated 1200 random numbers using the 'random' package in
Python and performed Gap test, Permutation test, and Run test. The
results also do not reject the null hypothesis.

5.  

    (a) Use the search engine to download the first one million digits
        of pi (for example, <https://www.piday.org/million/>) and check
        via graphic tools if the numbers violate the assumption of
        random numbers.

    (b) Apply the appropriate tools to test if the random numbers from

        (a) satisfy the assumption of random numbers.

```{r}
library(randtoolbox)
```

```{r}
pi1M <- readLines("Pi1MDP.txt", warn = FALSE) %>% 
  strsplit(split = "") %>% 
  unlist() %>% 
  as.integer()
```

```{r}
isRandom <- function(seq, alpha = 0.05){
  tb <- table(pi1M)
  testChi <- tb %>% chisq.test
  print(testChi)
  isUnif <- ifelse(testChi[["p.value"]] < alpha, FALSE, TRUE)
  
  cat(rep("*", 60), sep = "")
  
  r <- diff(range(seq))
  isIndept <- ifelse(gap.test(seq/r, echo = TRUE)[["p.value"]] < alpha, 
                     FALSE, TRUE)
  
  return(isUnif & isIndept)
}
```

```{r}
isRandom(pi1M, alpha = 0.05)
```

```{r}
pi1M <- readLines("Pi1MDP.txt", warn = FALSE) %>% 
  strsplit(split = "") %>% 
  unlist() %>% 
  as.integer()
pi1M + 1
TEST <- gap.test(pi1M/10, 0.25, 0.75)
result_vector <- numeric()
for (i in c(0:16)) {
  a = 0.5*0.5^i
  result_vector <- c(result_vector, a)
}
result_vector[17] <- 1 - sum(result_vector[1:16])
TEST <- c(TEST[1:16], sum(TEST[17:19]))
chisq.test(TEST, p = result_vector)
```

6.  The following table shows the winning numbers of first 20 Taiwan
    Lottery (starting in 2002), which picks 6 numbers from 42 balls plus
    a "Power Ball." Choose your tools to check whether these winning
    numbers are random.

```{r}
lottery_numbers <- c(
  22, 31, 34, 25, 21, 19,
  5, 18, 25, 26, 35, 42,
  32, 21, 9, 27, 31, 6,
  5, 25, 2, 16, 32, 9,
  15, 29, 5, 36, 13, 10,
  36, 16, 12, 26, 8, 34,
  4, 40, 27, 21, 14, 5,
  29, 4, 10, 23, 39, 14,
  30, 12, 40, 32, 35, 20,
  40, 6, 20, 29, 38, 35,
  32, 10, 15, 2, 30, 23,
  24, 20, 36, 19, 7, 12,
  1, 6, 7, 12, 42, 20,
  25, 39, 20, 38, 29, 37,
  26, 2, 15, 29, 4, 33,
  17, 39, 3, 15, 11, 1,
  13, 39, 28, 30, 25, 29,
  7, 9, 29, 34, 39, 36,
  28, 31, 16, 35, 6, 30,
  10, 32, 13, 4, 9, 33)
interval <- cut(lottery_numbers, breaks = seq(0, 42, by = 6))
hist(lottery_numbers, breaks = seq(0, 50, by = 6), main = "Histogram of Lottery Numbers", xlab = "Interval")
```

We divided the lottery numbers into groups of 6 digits each and plotted
a histogram. From the results, we can see that there is not much
difference in the quantity between each group.

```{r}
chisq.test(table(interval), p = rep(1/7, 7))
set.seed(123) 
gap_counts <- gap.test(lottery_numbers/42, 0.25, 0.75)
result_vector <- numeric()
for (i in c(0:3)) {
  a = 0.5*0.5^i
  result_vector <- c(result_vector, a)
}
result_vector[4] <- 1 - sum(result_vector[1:3])
observation <- c(25, 16,  11,  6) 
chisq.test(observation, p = result_vector)
```

Next, we examined the uniformity and independence of the sample using
both Chi-square test and Gap test. For the Chi-square test, we divided
the numbers into blocks of 6 digits each, and for the Gap test, the
threshold was set to 0.25 and 0.75. The results of both tests indicate
that when $\alpha$ = 0.05, we do not reject the null hypothesis.
Therefore, we conclude that these lottery numbers are random.
